{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Based Learning Applied to Document Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [从数据中学习](#从数据中学习)\n",
    "- [基于梯度的学习](#基于梯度的学习)\n",
    "- [梯度反传](#梯度反传)\n",
    "- [卷积神经网络用于单个字符识别](#卷积神经网络用于单个字符识别)\n",
    "  - [卷积网络](#卷积网络)\n",
    "  - [LeNet-5](#LeNet-5)\n",
    "  - [损失函数](#损失函数)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从数据中学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **数据集**：\n",
    "数据集可分为**训练数据集**与**测试数据集**，其中训练集用于学习算法，测试集用于评价学习到的算法的性能,\n",
    "\\begin{equation}\n",
    "\\label{eq:dataset}\n",
    "\\tag{式 1}\n",
    "{\\{(Z^1, D^1), \\cdots, (Z^P, D^P)\\}},\n",
    "\\end{equation}\n",
    "其中$(Z^i, D^i)$代表数据集中的第$i$个输入模式与期望的标记；\n",
    "\n",
    "2. **学习函数**：\n",
    "\\begin{equation}\n",
    "\\label{eq:learn function}\n",
    "\\tag{式 2}\n",
    "Y^P = F(Z^P, W),\n",
    "\\end{equation}\n",
    "其中$W$是需要学习的参数，$Y^P$代表算法在参数$W$的条件下预测输出;\n",
    "\n",
    "3. **损失函数**：\n",
    "\\begin{equation}\n",
    "\\label{eq:loss function}\n",
    "\\tag{式 3}\n",
    "E^P = \\mathcal{D}(D^P, Y^P) = \\mathcal{D}(D^P, F(Z^P, W)),\n",
    "\\end{equation}\n",
    "计算算法输出与标记之间的差距，用来评估系统的性能；\n",
    "\n",
    "4. 学习算法的目的就是要通过_最小化_**平均训练误差**$E_{train}(W)$找到一个公式(\\ref{eq:learn function})合适的参数$W$;\n",
    "5. 相比于平均训练误差，我们更关心**泛化误差**$E_{test}(W)$;\n",
    "6. \n",
    "\\begin{equation}\n",
    "\\label{eq:gap function}\n",
    "\\tag{式 4}\n",
    "E_{test} - E_{train} = k(h/P)^{\\alpha},\n",
    "\\end{equation}\n",
    "其中$P$是训练样本的个数，$\\alpha$是一个介于$0.5\\sim1$之间的常数，$k$是一个常数，$h$代表学习算法的容量（复杂度）;\n",
    "7. 为了得到最小化的泛化误差(\\ref{eq:loss function})，所以需要平衡算法容量$h$与样本数目$P$，从而引出结构化风险(\\ref{eq:structural risk})最小化；\n",
    "8. 结构化风险：\n",
    "\\begin{equation}\n",
    "\\label{eq:structural risk}\n",
    "\\tag{式 5}\n",
    "E_{train} + \\beta H(W),\n",
    "\\end{equation}\n",
    "其中$H(W)$是正则化函数用来限制模型容量$h$，$\\beta$是常数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于梯度的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度更新参数：\n",
    "\\begin{equation}\n",
    "\\label{eq:parameters update with gradient}\n",
    "\\tag{式 6}\n",
    "W_k = W_{k-1} - \\epsilon\\frac{\\partial E(W)}{\\partial W},\n",
    "\\end{equation}\n",
    "其中$\\epsilon$是一个常数或者对角矩阵等。\n",
    "\n",
    "随机梯度下降算法，参数更新仅使用其中一个样本或者部分样本：\n",
    "\\begin{equation}\n",
    "\\label{eq:sotchastic gradient algorithm}\n",
    "\\tag{式 7}\n",
    "W_k = W_{k-1} - \\epsilon\\frac{\\partial E^{p_k}(W)}{\\partial W},\n",
    "\\end{equation}\n",
    "参数会在其收敛轨道的周围波动，但是却通常比常规的梯度下降算法和二阶算法收敛更快。式 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度反传"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 损失函数的局部极值不在是实践中的主要问题；\n",
    "2. 反向传播算法：在多层非线性系统中计算梯度；\n",
    "3. 反向传播应用于使用sigmoidal单元多层神经网络能够解决复杂的问题。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络用于单个字符识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统神经网络用于图像识别存在的问题：\n",
    "1. 图像非常大，拥有很多的像素，全连接需要跟多参数，需要更多的训练样本\n",
    "2. 硬件内存要求太大\n",
    "3. 对于图像和语音信号，没有自带局部不变性\n",
    "4. 全连接网络在许多不同位置上会有很多的重复的权值\n",
    "5. 全连接网络完全忽略了输入的拓扑结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积特性：\n",
    "1. 局部连接\n",
    "2. 权值共享\n",
    "3. 空间（时间）采样\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lenet5](Gradient-Based-Learning-Applied-to-Document-Recognition/lenet5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结构解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C1：  \n",
    "    输入shape：$1\\times32\\times32$  \n",
    "    卷积核：$5\\times5$  \n",
    "    输出shape：$6\\times28\\times28$  \n",
    "    可训练参数：$(1\\times5\\times5+1)\\times6=156$  \n",
    "    连接数：$1\\times(5\\times5+1)\\times6\\times(28\\times28)=122304$  \n",
    "    说明：  \n",
    "     对输入图像进行第一次卷积运算（使用 $6$ 个大小为 $5\\times5$ 的卷积核），得到$6$个C1特征图（$6$个大小为$28\\times28$的 feature maps, $32-5+1=28$）。我们再来看看需要多少个参数，卷积核的大小为$5\\times5$，总共就有$6 \\times（5\\times5+1）=156$个参数，其中$+1$是表示一个核有一个bias。对于卷积层C1，C1内的每个像素都与输入图像中的$5\\times5$个像素和$1$个bias有连接，所以总共有$156\\times28\\times28=122304$个连接（connection）。有$122304$个连接，但是我们只需要学习$156$个参数，主要是通过权值共享实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S2:   \n",
    "    输入shape：$6\\times28\\times28$  \n",
    "    采样核：$2\\times2$  \n",
    "    输出shape：$6\\times14\\times14$  \n",
    "    可训练参数：$\\times(1+1)\\times6=12$  \n",
    "    连接数：$(2\\times2+1)\\times6\\times(14\\times14)=5880$   \n",
    "    说明：  \n",
    "     第一次卷积之后紧接着就是池化运算，使用 $2\\times2$核 进行池化，于是得到了S2，$6$个$14\\times14$的 特征图$(28/2=14)$。S2这个pooling层是对C1中的$2\\times2$区域内的像素求和乘以一个权值系数再加上一个偏置，然后将这个结果再做一次映射。于是每个池化核有两个训练参数，所以共有$2\\times6=12$个训练参数，但是有$5\\times14\\times14\\times6=5880$个连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C3:\n",
    "    输入shape：$6\\times14\\times14$  \n",
    "    卷积核：$5\\times5$  \n",
    "    输出shape：$16\\times10\\times10$  \n",
    "    可训练参数：$[3\\times(5\\times5)+1]\\times6+[4\\times(5\\times5)+1]\\times(6+3)+[6\\times(5\\times5)+1]\\times1=1516$  \n",
    "    连接数：$1516\\times(10\\times10)=151600$  \n",
    "    说明：第一次池化之后是第二次卷积，第二次卷积的输出是C3，$16$个$10\\times10$的特征图，卷积核大小是 $5\n",
    "    \\times5$. 我们知道S2 有$6$个 $14\\times14$ 的特征图，怎么从$6$个特征图得到$ 16$个特征图了？ 这里是通过对S2 的特征图特殊组合计算得到的$16$个特征图。具体如下：\n",
    "     ![c3与s2连接图](Gradient-Based-Learning-Applied-to-Document-Recognition/lenet5-feature-connect.png)\n",
    "     C3的前$6$个feature map（对应上图第一个红框的$6$列）与S2层相连的$3$个feature map相连接（上图第一个红框），后面$6$个feature map与S2层相连的$4$个feature map相连接（上图第二个红框），后面$3$个feature map与S2层部分不相连的$4$个feature map相连接，最后一个与S2层的所有feature map相连。卷积核大小依然为$5\\times5$，所以总共有$[3\\times(5\\times5)+1]\\times6+[4\\times(5\\times5)+1]\\times(6+3)+[6\\times(5\\times5)+1]\\times1=1516$个参数。而图像大小为$10\\times10$，所以共有$151600$个连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S4:  \n",
    "    输入shape：$16\\times10\\times10$  \n",
    "    采样核：$2\\times2$  \n",
    "    输出shape：$16\\times5\\times5$  \n",
    "    可训练参数：$\\times(1+1)\\times16=32$  \n",
    "    连接数：$(2\\times2+1)\\times16\\times(5\\times5)=2000$   \n",
    "    说明：\n",
    "    S4是pooling层，窗口大小仍然是$2\\times2$，共计$16$个feature map，C3层的$16$个$10\\times10$的图分别进行以$2\\times2$为单位的池化得到$16$个$5\\times5$的特征图。这一层有$2\\times16$共$32$个训练参数，$5\\times5\\times5\\times16=2000$个连接。连接的方式与S2层类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C5:  \n",
    "    输入shape：$16\\times5\\times5$  \n",
    "    卷积核：$5\\times5$  \n",
    "    输出shape：$120\\times1\\times1$  \n",
    "    可训练参数：$(16\\times5\\times5+1)\\times120=48120$  \n",
    "    连接数：$(16\\times5\\times5+1)\\times120\\times(1\\times1)=48120$  \n",
    "    说明：\n",
    "    C5层是一个卷积层。由于S4层的$16$个图的大小为$5\\times5$，与卷积核的大小相同，所以卷积后形成的图的大小为$1\\times1$。这里形成$120$个卷积结果。每个都与上一层的$16$个图相连。所以共有(5x5x16+1)x120 = 48120个参数，同样有48120个连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F6:\n",
    "    输入：c5 $120$维向量\n",
    "    计算方式：计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过sigmoid函数输出。\n",
    "    可训练参数：$84\\times(120+1)=10164$\n",
    "    说明：$6$层是全连接层。F6层有$84$个节点，对应于一个$7\\times12$的比特图，$-1$表示白色，$1$表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是$(120 + 1)\\times84=10164$。ASCII编码图如下：\n",
    "    ![ASCII编码图](Gradient-Based-Learning-Applied-to-Document-Recognition/lenet5-ascii-bitmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OUTPUT:  \n",
    "    Output层也是全连接层，共有$10$个节点，分别代表数字$0$到$9$，且如果节点$i$的值为$0$，则网络识别的结果是数字$i$。采用的是径向基函数（RBF）的网络连接方式。假设$x$是上一层的输入，$y$是RBF的输出，则RBF输出的计算方式是：\n",
    "    \\begin{equation}\n",
    "    \\label{eq:RBF}\n",
    "    \\tag{式 8}\n",
    "    y_i = \\sum_j(x_j - w_{ij})^2,\n",
    "    \\end{equation}\n",
    "    上式$w_ij$ 的值由$i$的比特图编码确定，$i$从$0$到$9$，$j$取值从$0$到$7\\times12-1$。RBF输出的值越接近于$0$，则越接近于$i$，即越接近于$i$的ASCII编码图，表示当前网络输入的识别结果是字符$i$。该层有$84\\times10=840$个参数和连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE\n",
    "\n",
    "\\begin{align}\n",
    "\\label{eq:loss MSE}\n",
    "    E(W) &= \\frac{1}{P}\\sum_{p=1}^{P}y_{D^p}(Z^p, W)\\\\\n",
    "    \\tag{式 9}\n",
    "    &=\\frac{1}{P}\\sum_{p=1}^{P}(Z^p == j)\\sum_j(x_j - w_{ij})^2\n",
    "\\end{align}\n",
    "\n",
    "注释：$y_{D^p}$代表输出的第$D_p-th$个单元是正确的类别，在大多数情况下该损失函数适合，但是缺少了三个重要的特性：\n",
    "- 如果允许径向基（RBF）的参数该表，$E(W)$将是不可接受的，例如：  \n",
    "     所有的RBF参数向量是相等的，并且其输入等于参数向量，那么其输出讲会是$0$，如果不允许参数改变则不会发生；\n",
    "- 类别之间没有竞争\n",
    "\n",
    "#### MAP\n",
    "\\begin{align}\n",
    "\\label{eq:loss}\n",
    "    \\tag{式 9}\n",
    "    E(W) &= \\frac{1}{P}\\sum_{p=1}^{P}(y_{D^p}(Z^p, W) + log(e^{-j} + \\sum_ie^{-y_i}(Z^p, W))),\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
